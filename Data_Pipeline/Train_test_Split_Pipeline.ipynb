{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71416ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a5f0f",
   "metadata": {},
   "source": [
    "### Open the hashmap file for train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6cd3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "landfil_locations_filename = 'waste_atlas_data/waste_atlas_latLongs.csv'\n",
    "main_directory_path = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker'\n",
    "\n",
    "hashed_locations = {} # if file is not available, instnatiate wit a blank dictionary\n",
    "\n",
    "filepath = os.path.join(main_directory_path, landfil_locations_filename) # path of the location data file\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "for index in range(len(df)):\n",
    "    lat = float(df.loc[index, \"Latitude\"]) # \"Lattitude column name is hard coded\"\n",
    "    long = float(df.loc[index, \"Longitude\"]) # \"Longitude column name is hard coded\"\n",
    "        \n",
    "    if (not lat) or (not long):\n",
    "        continue\n",
    "\n",
    "    _tup = (long, lat)\n",
    "    hashmap = hash(_tup) # Create the hash of the tuple (long, lat)\n",
    "        \n",
    "    hashed_locations[hashmap] = _tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1890e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker'\n",
    "# hashdict_filename = 'landfill_hashdict.txt'\n",
    "\n",
    "path = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c368b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashdict(path, hashdict_filename):\n",
    "    try: \n",
    "        with open(os.path.join(path, hashdict_filename), \"rb\") as hashed_l: # check if the hashed location file exists  \n",
    "                    hashed_locations = pickle.load(hashed_l) # load dictionary object\n",
    "        return hashed_locations\n",
    "    except:\n",
    "        print(\"Check the filename and filepath: not found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea843439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash_dict = get_hashdict(path=path, hashdict_filename=hashdict_filename)\n",
    "hash_dict = hashed_locations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bcc6037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hash_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba41e07",
   "metadata": {},
   "source": [
    "### Split the hashmaps in train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d47762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(d, test_size=0.05):\n",
    "    total_size = len(d)\n",
    "    \n",
    "    if type(test_size) == float:\n",
    "        size = int(total_size * test_size)\n",
    "    elif type(test_size) == int:\n",
    "        size = test_size\n",
    "    else:\n",
    "        print(\"please check the test size parameter\")\n",
    "        return\n",
    "    keys = random.sample(list(d.keys()), size)\n",
    "    \n",
    "    test_sample = {k: d[k] for k in keys}\n",
    "    train_sample = {k:d[k] for k in d.keys() if k not in keys}\n",
    "    \n",
    "    return train_sample, test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de902344",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hashmap, test_hashmap = create_train_test_sets(hash_dict, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "057e7e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 457\n",
      "Test set size: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size: {}\".format(len(train_hashmap)))\n",
    "print(\"Test set size: {}\".format(len(test_hashmap)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282563b",
   "metadata": {},
   "source": [
    "### Saving the train and test hashmap for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "093199ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_test_dict (path, datadict ,filenames = ['training_datadict.pkl', 'test_datadict.pkl']):\n",
    "    \"\"\"\n",
    "    path: str(path where we want to save the dictionaries)\n",
    "    datadict: list(dict) (list of dictionaries containing traning and test dict)\n",
    "    filenames: list(str) (list of training and test filenames)\n",
    "    \n",
    "    return None\n",
    "    \"\"\"\n",
    "    \n",
    "    for itr, filename in enumerate(filenames):\n",
    "        with open(os.path.join(path, filename), \"wb\") as fw:\n",
    "            pickle.dump(datadict[itr], fw, protocol=pickle.HIGHEST_PROTOCOL) # rewrite the most updated file to disk \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "712f2935",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_train_test_dict(path = path, datadict = [train_hashmap, test_hashmap])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a459c50",
   "metadata": {},
   "source": [
    "#### Saving the additional data into old train and test hash maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1e000f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_dict_name = 'training_datadict.pkl'\n",
    "test_data_dict_name = 'test_datadict.pkl'\n",
    "\n",
    "with open(os.path.join(path, training_data_dict_name), \"rb\") as training_loc:  \n",
    "    train_locs = pickle.load(training_loc)\n",
    "    \n",
    "with open(os.path.join(path, test_data_dict_name), \"rb\") as test_loc:  \n",
    "    test_locs = pickle.load(test_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "308e1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_dict = {**train_locs, **train_hashmap}\n",
    "full_test_dict = {**test_locs, **test_hashmap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48a59ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2521"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_training_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8c4e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_train_test_dict(path = path, \n",
    "                     datadict = [full_training_dict, full_test_dict], \n",
    "                     filenames = ['full_training_datadict.pkl', 'full_test_datadict.pkl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09482009",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "c = 0\n",
    "for k, v in train_hashmap.items():\n",
    "    if c <= 101:\n",
    "        d[k] = 'lalit'\n",
    "    elif c <= 203:\n",
    "        d[k] = 'sonya'\n",
    "    elif c <= 305:\n",
    "        d[k] = 'brian'\n",
    "    elif c <= 407:\n",
    "        d[k] = 'michael'\n",
    "    else:\n",
    "        d[k] = 'prakhar'\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe864a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a939358",
   "metadata": {},
   "source": [
    "### Moving training and test files in appropriate folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0ee51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker/training_data/'\n",
    "test_data_path = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker/test_data/'\n",
    "base_image_repo = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker/complete_image_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b71e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(img_name):\n",
    "    elems = img_name.split('_')\n",
    "    if len(elems) == 2:\n",
    "        return int(elems[1][:-4])\n",
    "    else:\n",
    "        return int(elems[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a47d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_destination_folder(name):\n",
    "    if name == 'lalit':\n",
    "        dest = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker/data_tagging/lalit/training'\n",
    "    elif name == 'sonya':\n",
    "        dest = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker/data_tagging/sonya/training'\n",
    "    elif name == 'bryan':\n",
    "        dest = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker/data_tagging/bryan/training'\n",
    "    elif name == 'michael':\n",
    "        dest = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker/data_tagging/michael/training'\n",
    "    else:\n",
    "        dest = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker/data_tagging/prakhar/training'\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d50432c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_train_test_tag_data(base_image_repo, train_data_path, test_data_path, test_datadict, map_dict):\n",
    "    \n",
    "    for itr, fname in enumerate(os.listdir(base_image_repo)):\n",
    "        \n",
    "        if itr % 10 == 0:\n",
    "            print('{} files complete'.format(itr))\n",
    "\n",
    "        hashmap = get_hash(fname)\n",
    "        src = base_image_repo+fname\n",
    "\n",
    "        if hashmap in test_datadict:\n",
    "            shutil.copy(src=src, \n",
    "                        dst='D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker/data_tagging/prakhar/test')\n",
    "            dest = test_data_path+fname\n",
    "            os.rename(src, dest)\n",
    "        else:\n",
    "            copy_dest = get_destination_folder(map_dict[hashmap])\n",
    "            \n",
    "            if fname.startswith('BASE'):\n",
    "                shutil.copy(src=src, dst=copy_dest)\n",
    "            \n",
    "            dest = train_data_path+fname\n",
    "            os.rename(src, dest)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2eb3de47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files complete\n",
      "10 files complete\n",
      "20 files complete\n",
      "30 files complete\n",
      "40 files complete\n",
      "50 files complete\n",
      "60 files complete\n",
      "70 files complete\n",
      "80 files complete\n",
      "90 files complete\n",
      "100 files complete\n",
      "110 files complete\n",
      "120 files complete\n",
      "130 files complete\n",
      "140 files complete\n",
      "150 files complete\n",
      "160 files complete\n",
      "170 files complete\n",
      "180 files complete\n",
      "190 files complete\n",
      "200 files complete\n",
      "210 files complete\n",
      "220 files complete\n",
      "230 files complete\n",
      "240 files complete\n",
      "250 files complete\n",
      "260 files complete\n",
      "270 files complete\n",
      "280 files complete\n",
      "290 files complete\n",
      "300 files complete\n",
      "310 files complete\n",
      "320 files complete\n",
      "330 files complete\n",
      "340 files complete\n",
      "350 files complete\n",
      "360 files complete\n",
      "370 files complete\n",
      "380 files complete\n",
      "390 files complete\n",
      "400 files complete\n",
      "410 files complete\n",
      "420 files complete\n",
      "430 files complete\n",
      "440 files complete\n",
      "450 files complete\n",
      "460 files complete\n",
      "470 files complete\n",
      "480 files complete\n",
      "490 files complete\n",
      "500 files complete\n",
      "510 files complete\n",
      "520 files complete\n",
      "530 files complete\n",
      "540 files complete\n",
      "550 files complete\n",
      "560 files complete\n",
      "570 files complete\n",
      "580 files complete\n",
      "590 files complete\n",
      "600 files complete\n",
      "610 files complete\n",
      "620 files complete\n",
      "630 files complete\n",
      "640 files complete\n",
      "650 files complete\n",
      "660 files complete\n",
      "670 files complete\n",
      "680 files complete\n",
      "690 files complete\n",
      "700 files complete\n",
      "710 files complete\n",
      "720 files complete\n",
      "730 files complete\n",
      "740 files complete\n",
      "750 files complete\n",
      "760 files complete\n",
      "770 files complete\n",
      "780 files complete\n",
      "790 files complete\n",
      "800 files complete\n",
      "810 files complete\n",
      "820 files complete\n",
      "830 files complete\n",
      "840 files complete\n",
      "850 files complete\n",
      "860 files complete\n",
      "870 files complete\n",
      "880 files complete\n",
      "890 files complete\n",
      "900 files complete\n",
      "910 files complete\n",
      "920 files complete\n",
      "930 files complete\n",
      "940 files complete\n",
      "950 files complete\n",
      "960 files complete\n",
      "970 files complete\n",
      "980 files complete\n",
      "990 files complete\n",
      "1000 files complete\n",
      "1010 files complete\n",
      "1020 files complete\n",
      "1030 files complete\n",
      "1040 files complete\n",
      "1050 files complete\n",
      "1060 files complete\n",
      "1070 files complete\n",
      "1080 files complete\n",
      "1090 files complete\n",
      "1100 files complete\n",
      "1110 files complete\n",
      "1120 files complete\n",
      "1130 files complete\n",
      "1140 files complete\n",
      "1150 files complete\n",
      "1160 files complete\n",
      "1170 files complete\n",
      "1180 files complete\n",
      "1190 files complete\n",
      "1200 files complete\n",
      "1210 files complete\n",
      "1220 files complete\n",
      "1230 files complete\n",
      "1240 files complete\n",
      "1250 files complete\n",
      "1260 files complete\n",
      "1270 files complete\n",
      "1280 files complete\n",
      "1290 files complete\n",
      "1300 files complete\n",
      "1310 files complete\n",
      "1320 files complete\n",
      "1330 files complete\n",
      "1340 files complete\n",
      "1350 files complete\n",
      "1360 files complete\n",
      "1370 files complete\n",
      "1380 files complete\n",
      "1390 files complete\n",
      "1400 files complete\n",
      "1410 files complete\n",
      "1420 files complete\n",
      "1430 files complete\n",
      "1440 files complete\n",
      "1450 files complete\n",
      "1460 files complete\n",
      "1470 files complete\n",
      "1480 files complete\n",
      "1490 files complete\n",
      "1500 files complete\n",
      "1510 files complete\n",
      "1520 files complete\n",
      "1530 files complete\n",
      "1540 files complete\n",
      "1550 files complete\n",
      "1560 files complete\n",
      "1570 files complete\n",
      "1580 files complete\n",
      "1590 files complete\n",
      "1600 files complete\n",
      "1610 files complete\n",
      "1620 files complete\n",
      "1630 files complete\n",
      "1640 files complete\n",
      "1650 files complete\n",
      "1660 files complete\n",
      "1670 files complete\n",
      "1680 files complete\n",
      "1690 files complete\n",
      "1700 files complete\n",
      "1710 files complete\n",
      "1720 files complete\n",
      "1730 files complete\n",
      "1740 files complete\n",
      "1750 files complete\n",
      "1760 files complete\n",
      "1770 files complete\n",
      "1780 files complete\n",
      "1790 files complete\n",
      "1800 files complete\n",
      "1810 files complete\n",
      "1820 files complete\n",
      "1830 files complete\n",
      "1840 files complete\n",
      "1850 files complete\n",
      "1860 files complete\n",
      "1870 files complete\n",
      "1880 files complete\n",
      "1890 files complete\n",
      "1900 files complete\n",
      "1910 files complete\n",
      "1920 files complete\n",
      "1930 files complete\n",
      "1940 files complete\n",
      "1950 files complete\n",
      "1960 files complete\n",
      "1970 files complete\n",
      "1980 files complete\n",
      "1990 files complete\n",
      "2000 files complete\n",
      "2010 files complete\n",
      "2020 files complete\n",
      "2030 files complete\n",
      "2040 files complete\n",
      "2050 files complete\n",
      "2060 files complete\n",
      "2070 files complete\n",
      "2080 files complete\n",
      "2090 files complete\n",
      "2100 files complete\n",
      "2110 files complete\n",
      "2120 files complete\n",
      "2130 files complete\n",
      "2140 files complete\n",
      "2150 files complete\n",
      "2160 files complete\n",
      "2170 files complete\n",
      "2180 files complete\n",
      "2190 files complete\n",
      "2200 files complete\n",
      "2210 files complete\n",
      "2220 files complete\n",
      "2230 files complete\n",
      "2240 files complete\n",
      "2250 files complete\n",
      "2260 files complete\n",
      "2270 files complete\n",
      "2280 files complete\n",
      "2290 files complete\n",
      "2300 files complete\n",
      "2310 files complete\n",
      "2320 files complete\n",
      "2330 files complete\n",
      "2340 files complete\n",
      "2350 files complete\n",
      "2360 files complete\n",
      "2370 files complete\n",
      "2380 files complete\n",
      "2390 files complete\n",
      "2400 files complete\n",
      "2410 files complete\n",
      "2420 files complete\n",
      "2430 files complete\n",
      "2440 files complete\n",
      "2450 files complete\n",
      "2460 files complete\n",
      "2470 files complete\n",
      "2480 files complete\n",
      "2490 files complete\n",
      "2500 files complete\n",
      "2510 files complete\n",
      "2520 files complete\n",
      "2530 files complete\n",
      "2540 files complete\n",
      "2550 files complete\n",
      "2560 files complete\n",
      "2570 files complete\n",
      "2580 files complete\n",
      "2590 files complete\n",
      "2600 files complete\n",
      "2610 files complete\n",
      "2620 files complete\n",
      "2630 files complete\n",
      "2640 files complete\n",
      "2650 files complete\n",
      "2660 files complete\n",
      "2670 files complete\n",
      "2680 files complete\n",
      "2690 files complete\n",
      "2700 files complete\n",
      "2710 files complete\n",
      "2720 files complete\n",
      "2730 files complete\n",
      "2740 files complete\n",
      "2750 files complete\n",
      "2760 files complete\n",
      "2770 files complete\n",
      "2780 files complete\n",
      "2790 files complete\n",
      "2800 files complete\n",
      "2810 files complete\n",
      "2820 files complete\n",
      "2830 files complete\n",
      "2840 files complete\n",
      "2850 files complete\n",
      "2860 files complete\n",
      "2870 files complete\n",
      "2880 files complete\n",
      "2890 files complete\n"
     ]
    }
   ],
   "source": [
    "move_train_test_tag_data(base_image_repo=base_image_repo, \n",
    "                         train_data_path=train_data_path, \n",
    "                         test_data_path=test_data_path, \n",
    "                         test_datadict=full_test_dict, \n",
    "                         map_dict=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd0cae",
   "metadata": {},
   "source": [
    "### Old pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1eec288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_train_test_data(base_image_repo, train_data_path, test_data_path, test_datadict, correct_filename=False):\n",
    "    \n",
    "    for itr, fname in enumerate(os.listdir(base_image_repo)):\n",
    "        \n",
    "        if itr % 100 == 0:\n",
    "            print('{} files complete'.format(itr))\n",
    "        \n",
    "        #print(fname)\n",
    "    \n",
    "        if correct_filename:\n",
    "            corrected_fname = fname[:-4]\n",
    "\n",
    "        hashmap = get_hash(fname)\n",
    "        src = base_image_repo+fname\n",
    "\n",
    "        if hashmap in test_datadict:\n",
    "            dest = test_data_path+corrected_fname\n",
    "            os.rename(src, dest)\n",
    "        else:\n",
    "            dest = train_data_path+corrected_fname\n",
    "            os.rename(src, dest)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5dec0d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files complete\n",
      "100 files complete\n",
      "200 files complete\n",
      "300 files complete\n",
      "400 files complete\n",
      "500 files complete\n",
      "600 files complete\n",
      "700 files complete\n",
      "800 files complete\n",
      "900 files complete\n",
      "1000 files complete\n",
      "1100 files complete\n",
      "1200 files complete\n",
      "1300 files complete\n",
      "1400 files complete\n",
      "1500 files complete\n",
      "1600 files complete\n",
      "1700 files complete\n",
      "1800 files complete\n",
      "1900 files complete\n",
      "2000 files complete\n",
      "2100 files complete\n",
      "2200 files complete\n",
      "2300 files complete\n",
      "2400 files complete\n",
      "2500 files complete\n",
      "2600 files complete\n",
      "2700 files complete\n",
      "2800 files complete\n",
      "2900 files complete\n",
      "3000 files complete\n",
      "3100 files complete\n",
      "3200 files complete\n",
      "3300 files complete\n",
      "3400 files complete\n",
      "3500 files complete\n",
      "3600 files complete\n",
      "3700 files complete\n",
      "3800 files complete\n",
      "3900 files complete\n",
      "4000 files complete\n",
      "4100 files complete\n",
      "4200 files complete\n",
      "4300 files complete\n",
      "4400 files complete\n",
      "4500 files complete\n",
      "4600 files complete\n",
      "4700 files complete\n",
      "4800 files complete\n",
      "4900 files complete\n",
      "5000 files complete\n",
      "5100 files complete\n",
      "5200 files complete\n",
      "5300 files complete\n",
      "5400 files complete\n",
      "5500 files complete\n",
      "5600 files complete\n",
      "5700 files complete\n",
      "5800 files complete\n",
      "5900 files complete\n",
      "6000 files complete\n",
      "6100 files complete\n",
      "6200 files complete\n",
      "6300 files complete\n",
      "6400 files complete\n",
      "6500 files complete\n",
      "6600 files complete\n",
      "6700 files complete\n",
      "6800 files complete\n",
      "6900 files complete\n",
      "7000 files complete\n",
      "7100 files complete\n",
      "7200 files complete\n",
      "7300 files complete\n",
      "7400 files complete\n",
      "7500 files complete\n",
      "7600 files complete\n",
      "7700 files complete\n",
      "7800 files complete\n",
      "7900 files complete\n",
      "8000 files complete\n",
      "8100 files complete\n",
      "8200 files complete\n",
      "8300 files complete\n",
      "8400 files complete\n",
      "8500 files complete\n",
      "8600 files complete\n",
      "8700 files complete\n",
      "8800 files complete\n",
      "8900 files complete\n",
      "9000 files complete\n",
      "9100 files complete\n",
      "9200 files complete\n",
      "9300 files complete\n",
      "9400 files complete\n",
      "9500 files complete\n",
      "9600 files complete\n",
      "9700 files complete\n",
      "9800 files complete\n",
      "9900 files complete\n",
      "10000 files complete\n",
      "10100 files complete\n",
      "10200 files complete\n",
      "10300 files complete\n",
      "10400 files complete\n",
      "10500 files complete\n",
      "10600 files complete\n",
      "10700 files complete\n",
      "10800 files complete\n",
      "10900 files complete\n",
      "11000 files complete\n",
      "11100 files complete\n",
      "11200 files complete\n",
      "11300 files complete\n",
      "11400 files complete\n",
      "11500 files complete\n",
      "11600 files complete\n",
      "11700 files complete\n",
      "11800 files complete\n",
      "11900 files complete\n",
      "12000 files complete\n",
      "12100 files complete\n",
      "12200 files complete\n",
      "12300 files complete\n",
      "12400 files complete\n",
      "12500 files complete\n",
      "12600 files complete\n",
      "12700 files complete\n",
      "12800 files complete\n",
      "12900 files complete\n",
      "13000 files complete\n"
     ]
    }
   ],
   "source": [
    "move_train_test_data(base_image_repo=base_image_repo, \n",
    "                     train_data_path=train_data_path, \n",
    "                     test_data_path=test_data_path, \n",
    "                     test_datadict=test_hashmap, \n",
    "                     correct_filename=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
