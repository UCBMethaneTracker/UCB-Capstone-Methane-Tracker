{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./graphics/RUS_logo.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>  Monitoring tropospheric NO2 with Sentinel-5P products using the Atmospheric Toolbox - Part 1      </h1></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**General Note 1**: Execute each cell through the <button class=\"btn btn-default btn-xs\"><i class=\"icon-play fa fa-play\"></i></button> button from the top MENU (or keyboard shortcut `Shift` + `Enter`).<br>\n",
    "<br>\n",
    "**General Note 2**: If, for any reason, the kernel is not working anymore, in the top MENU, click on the <button class=\"btn btn-default btn-xs\"><i class=\"fa fa-repeat icon-repeat\"></i></button> button. Then, in the top MENU, click on \"Cell\" and select \"Run All Above Selected Cell\".<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "- [1. Introduction](#1.-Introduction)\n",
    "- [2. Installing and importing python libraries](#2.-Installing-and-importing-python-libraries)\n",
    "- [3. Download Sentinel-5P data](#3.-Download-Sentinel-5P-data)\n",
    "- [4. Explore the netCDF file content](#4.-Explore-the-netCDF-file-content)\n",
    "  - [4.1. Product Exploration](#4.1.-Product-Exploration)\n",
    "  - [4.2. Visualisation of one NO2 product](#4.2.-Visualisation-of-one-NO2-product)\n",
    "- [5. Visualisation of NO2 for a day](#5.-Visualisation-of-NO2-for-a-day)\n",
    "  - [5.1. Concatenation](#5.1.-Concatenation)\n",
    "     - [5.1.1. Harpmerge in command line](#5.1.1.-Harpmerge-in-command-line)\n",
    "     - [5.1.2. Harp.import_product() without preprocessing](#5.1.2.-Harp.import_product()-without-preprocessing)\n",
    "     - [5.1.3. Harp.import_product() with preprocessing](#5.1.3.-Harp.import_product()-with-preprocessing)\n",
    "  - [5.2. Visualisation of NO2](#5.2.-Visualisation-of-NO2)\n",
    "     - [5.2.1. Visualisation of the three outputs](#5.2.1.-Visualisation-of-the-three-outputs)\n",
    "     - [5.2.2. Visualisation of NO2 over Europe](#5.2.2.-Visualisation-of-NO2-over-Europe)\n",
    "- [6. Conclusion](#6.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    " [Go back to the \"Table of contents\"](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "Air quality is one of the major threats to human health, accounting for 13% of the deaths in the European Union. Monitoring the air we breathe is key to developing mitigation strategies and quantify the effects of these policies. Pollutants are emitted from different sources, most of them being anthropogenic sources such as motor vehicles and industrial combustion processes. To implement new policies, authorities rely on space based technologies as they offer a unique way to monitor air pollutants daily and globally.\n",
    "\n",
    "<p style='text-align: justify;'> \n",
    "In this context, Sentinel-5P was launched in October 2017. Its main purpose is to screen the Earth’s atmosphere and quantify different pollutants (CO, NO<sub>2</sub>, SO<sub>2</sub>, O<sub>3</sub>, aerosols…) with a great accuracy and spatial resolution. It also provides measurement continuity with precedent and ongoing atmospheric spatial missions (OMI, IASI and SCHIAMACHY). The data recorded by this satellite are free of use and present a great interest to globally monitor air quality, greenhouse gas emissions and detect and assess the impact of polluting events.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> More information on the characteristics of Sentinel-5p can be found <a href=\"https://sentinel.esa.int/web/sentinel/missions/sentinel-5p\"> here </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "    This part of the training will focus on opening and exploring Sentinel-5P NO2 products with python. We will eventually apply some basic processing on the products to monitor NO2 in the troposphere (lower part of the atmosphere). The specifically designed <a href=\"https://atmospherictoolbox.org/\"> Atmospheric Toolbox </a> and the <a href=\"http://xarray.pydata.org/en/stable/\"> xarray </a> Python library will be used throughout the workshop. You will learn to monitor NO2 pollution based on processed Sentinel-5P data.\n",
    "\n",
    "<p style='text-align: justify;'> \n",
    "    In this first part, we are going to introduce you to ingesting, reading and navigating through Sentinel-5P NO2 products with Python. This will include: \n",
    "    \n",
    "* Downloading automatically S5P products by command line;\n",
    "* Opening and exploring the downloaded products;\n",
    "* Mapping the NO2 tropospheric column in one product;\n",
    "* Three different ways of concatenating NO2 products to create averaged maps;\n",
    "* Defining a specific study area and adapting the processing;\n",
    "    \n",
    "<p style='text-align: justify;'>\n",
    "    The exercises will be implemented using Python code that can be found in this Jupyter Notebook. Although it is recommended to have some basic knowledge, the training does not require any Python programming skills and can be followed by any participants. If you are new to Python and Jypyter Notebooks, here are some reference that can help you:\n",
    "\n",
    "* [Python Tutorial](https://www.python.org/about/gettingstarted/)\n",
    "* [Jupyter Notebook documentation](https://jupyter.readthedocs.io/en/latest/index.html)\n",
    "* [Jupyter Notebook tutorial](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Installing and importing python libraries\n",
    " [Go back to the \"Table of contents\"](#Table-of-contents)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b> Tip: Importing modules </b>\n",
    "* Python libraries need to be *imported* before they can be used;\n",
    "* Imported libraries usually have a namespace;\n",
    "* Portions of libraries, can be imported;\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All librairies used in this notebook are already installed on the `conda` environment we just creadted. We hence simply need to import the requested librairies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module import Error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # to ignore warning message\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    from matplotlib import pyplot as plt # Visualization\n",
    "    import cartopy.crs as ccrs # Projected visualizations\n",
    "    import xarray as xr # Open, read and Process netCDF files\n",
    "    import numpy as np # Data manupulation\n",
    "    import cartopy # improved visualizations\n",
    "    from glob import iglob # data access in file manager\n",
    "    import glob   # data access in file manager\n",
    "    from os.path import join # data access in file manager\n",
    "    import pandas as pd # data manipulation\n",
    "    import harp #Python interface of the atmospheric toolbox to open read and process Sentinel-5P products\n",
    "    import imageio # create gif \n",
    "    import matplotlib.dates as mdates # date manipulation\n",
    "    import matplotlib.patches as mpatches # to define patches (2D artist with a face color and an edge color such as rectangle, circles)\n",
    "    from matplotlib.dates import DateFormatter # date manipulation\n",
    "    import time #to manipulate time in Python and create timer\n",
    "    import os\n",
    "    \n",
    "except ModuleNotFoundError:\n",
    "    print ('Module import Error')\n",
    "else:\n",
    "    print('\\nAll librairies properly loaded. Ready to start!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download Sentinel-5P data\n",
    " [Go back to the \"Table of contents\"](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "    In the earlier presentation, you learned how to download products one by one from the <a href=\"https://s5phub.copernicus.eu/dhus/\"> Copernicus scihub </a> by manually looking for products matching your search criteria.\n",
    "<p style='text-align: justify;'>\n",
    "    The Copernicus Scihub also has an API to automatically retrieve data. This is useful when users need to download a large amount of products or download products on a regular basis (daily download of the products ingested in the last 24 hours for example). This section of the training introduces you to the Sentinel-5P data automatic retrieval process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> More information on the API hub can be found <a href=\"https://scihub.copernicus.eu/twiki/do/view/SciHubUserGuide/BatchScripting?redirectedfrom=SciHubUserGuide.8BatchScripting\"> here </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "In this example, we are going to download one NO2 product over Europe on March 24th 2019 using the API hub. To do so, we will use the dhusget.sh script which is an example script implemented for the API hub and freely available to any users of the Copernicus scihub.\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "First, you need to download the dhusget script <a href=\"https://scihub.copernicus.eu/twiki/pub/SciHubUserGuide/BatchScripting/dhusget.sh\"> here</a>. We have downloaded it beforehand of this training session. You will find it in `/shared/S5P_NO2_092021/training_NO2_22092021/AuxData/bash/dhusget.sh`.\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "This script can be run via a simple command line. It searches for products based on several options that allow you to specify your search criteria. The options used in this training section are the following:\n",
    "    \n",
    "<b> Login option </b>\n",
    " - -d \\<DHuS url\\> : URL of the Data Hub Service to be polled - here `https://s5phub.copernicus.eu/dhus`\n",
    " - -u \\<username\\> : data hub username - here `'s5pguest'`\n",
    " - -p \\<password\\> : data hub password - here `'s5pguest'`\n",
    " \n",
    "<b> Search query option </b>\n",
    " - -m \\<mission name\\> : sentinel mission name - here `'Sentinel-5'`\n",
    " - -S \\<sensing_date_FROM\\> : search for products with sensing date greater than the date and time specified by \\<sensing_date_FROM\\>. The date format is ISO 8601: YYYY-MM-DDThh:mm:ss.cccZ - here `'2019-03-24T00:00:00.000Z'`\n",
    " - -E \\<sensing_date_TO\\> : search for products with sensing date less than the date and time specified by \\<sensing_date_TO>\\. The date format is ISO 8601: YYYY-MM-DDThh:mm:ss.cccZ - here `'2019-03-24T20:00:00.000Z'`\n",
    " - -F \\<free OpenSearch query\\> : free text OpenSearch query. The query must be written enclosed by single apexes '<query>' (e.g. -F 'platformname:Sentinel-1 AND producttype:SLC' ). Note: the free text OpenSearch query can be combined with the other possible specified search options - here `'platformname:Sentinel-5 AND producttype:L2__NO2___ AND processinglevel:L2 AND processingmode:Offline'`\n",
    " - -c \\<coordinates i.e.: lon_min,lat_max:lon_max,lat_min\\> : coordinates of two opposite vertices of the rectangular area of interest - here `'8.3,42.1:8.2,42.0'`\n",
    "    \n",
    "<b> Search Result options </b>\n",
    " - -q \\<XMLfile\\> : write the OpenSearch query results in a specified XML file. Default file is './OSquery-result.xml' - here `/shared/S5P_NO2_092021/training_NO2_22092021/Processing/automatic_download/search_result_'date +%d%m%y\\'_S5P.xml` (be aware, we must specify a relative path from the Notebook)\n",
    " - -C \\<CSVfile\\> : write the list of product results in a specified CSV file. Default file is './products-list.csv' - here `/shared/S5P_NO2_092021/training_NO2_22092021/Processing/automatic_download/products_list_`date +%d%m%y`_S5P.csv` (be aware, we must specify a relative path from the Notebook)\n",
    " - -l \\<results\\> : maximum number of results per page [1,2,3,4,..100]; default value = 25 - here `100`\n",
    "    \n",
    "<b> Download options </b>\n",
    " - -O \\<path/filename\\> : save the product netCDF file in the specified folder with the specified filename. - here `/shared/S5P_NO2_092021/training_NO2_22092021/Processing/automatic_download/ (the folder gets created automatically if it does not exist)\n",
    " - -o \\<option\\> : what to download - here `'product'`\n",
    " \n",
    " \n",
    " Within the script `dhusget.sh` and the [website](https://scihub.copernicus.eu/twiki/do/view/SciHubUserGuide/BatchScripting#dhusget_script) (where the bash can be downloaded) there is a description of all available options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Tip:</b> When calling a linux command from a notebook, insert a `!` at the beginning of the command line. To split a long command line into several smaller ones, use `\\` at the end of each line.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command line to download data\n",
    "!/shared/S5P_NO2_092021/training_NO2_22092021/AuxData/bash/dhusget.sh \\\n",
    "-d 'https://s5phub.copernicus.eu/dhus' \\\n",
    "-u 's5pguest' -p 's5pguest' \\\n",
    "-m 'Sentinel-5' \\\n",
    "-S 2019-03-24T00:00:00.000Z -E 2019-03-24T20:00:00.000Z \\\n",
    "-F 'platformname:Sentinel-5 AND producttype:L2__NO2___ AND processinglevel:L2 AND processingmode:Offline' \\\n",
    "-l 100 \\\n",
    "-c 8.3,42.1:8.2,42.0 \\\n",
    "-q ../Processing/automatic_download/search_result_`date +%d%m%y`_S5P.xml \\\n",
    "-C ../Processing/automatic_download/products_list_`date +%d%m%y`_S5P.csv \\\n",
    "-O '/shared/S5P_NO2_092021/training_NO2_22092021/Processing/automatic_download/' \\\n",
    "-o 'product'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information, we have chosen a tiny delimiting box (8.3,42.1:8.2,42.0) to retrieve only one product during the session and spare some time downloading the products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The above cell is an example of how to automatically download S5P products. To spare time, we will use products that are already downloaded in the following parts of the notebook. If you want to dowload them later or download other products of your choice, you should modify the command line above! We will see a second example in [part 5.1](#5.1.-Concatenation)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explore the netCDF file content\n",
    " [Go back to the \"Table of contents\"](#Table-of-contents)\n",
    " \n",
    "<img style=\"float: left;\" src=graphics/S5p_FileStructre.PNG width='400'/>\n",
    "\n",
    "Before starting, it is important to remind how the Sentinel-5P netCDF files are structured (see above image). There are different groups used to organise the data and make it easier to find what you are looking for.\n",
    "\n",
    "There are two groups in each Sentinel-5P netCDF file: PRODUCT and METADATA which themselves contain other subgroups.\n",
    "* **PRODUCT**: This group stores the main fields (latitude, longitude, main variables). The `qa_value` parameter summarizes the processing flags into a continuous value, giving a quality percentage: 100% is the most optimal value, 0% is a failure, in between lies a continuum of values;\n",
    "* **METADATA**: This group collects metadata items. These metadata standards are all meant to facilitate dataset discovery. The metadata will be stored as attributes, while grouping attributes that belong to a specific standard will be done by using sub-groups in the Metadata group.\n",
    "\n",
    "In the following parts of this Notebook, we will use the <a href=\"http://xarray.pydata.org/en/stable/\"> `xarray` </a> Python library to open and manipulate the products. We will also use the <a href=\"https://atmospherictoolbox.org/\"> Atmospheric Toolbox </a> and its <a href=\"https://atmospherictoolbox.org/harp/\"> HARP </a> component to preprocess the files. \n",
    "\n",
    "One important thing to note is that `xarray`  doesn't handle NetCDF groups and groups must be imported one by one. So if you want to interactively discover all groups of a product and the variables they contain, we advise you to use `Panoply` as shown in the previous exercise (the application is installled in your virtual machine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Product Exploration\n",
    "\n",
    "In this part, we are going to work with only one file to get familiar with xarray and python commands.\n",
    "\n",
    "Let's check the content of the Sentinel-5P NO2 file we just downloaded. We first need to define:\n",
    "* `RootPath`: the path pointing to the data directory\n",
    "* `FName`: the path of the NetCDF file (`Rootpath`+filename.nc) \n",
    "\n",
    "These variables can of course be changed later on depending on the netCDF filename and where the data is stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define your data directory Rootpath and your filename FName\n",
    "RootPath = '/shared/S5P_NO2_092021/training_NO2_22092021/Processing/automatic_download/'\n",
    "FName = RootPath + 'S5P_OFFL_L2__NO2____20190324T111531_20190324T125701_07478_01_010300_20190330T125634.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have told Jupyter which data we are going to work with and where it is, we need to open this data.\n",
    "The following cell opens the netCDF file (`xr.open_dataset()` function) as an <a href=\"http://xarray.pydata.org/en/stable/data-structures.html#dataset\"> xarray.Dataset</a> and allows you to interactively browse the content of the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Tip:</b> To access the <strong>PRODUCT</strong> and <strong>METADATA</strong> group, the keyword `group` has to be passed as argument to the open_dataset() function as `xarray` does not support netCDF groups as part of the DataSet data model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open product\n",
    "xr.open_dataset(FName,group='PRODUCT')  # Ingest group 'PRODUCT' group into an xarray.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to store the product content into an xarray.Dataset called `FIn` and display the content of the dataset, you need to execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  Open the netCDF file with xr.open_dataset() and get general information ####\n",
    "\n",
    "## File\n",
    "FIn = xr.open_dataset(FName,group='PRODUCT')   #Handling of the netCDf file\n",
    "\n",
    "# Show Header: global attributes\n",
    "FIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print the different variables of a netCDF file ###\n",
    "FIn.data_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function displays all available variables in the `product` group of a file along with the dimensions they depend on and their data type (`float32` here, meaning that each value is a float coded on 32 bits). You can see that the present variables in the files depend on several coordinates. To get information about these coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Print the coordinates of a netCDF file ###\n",
    "FIn.coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to store the values of a variable in an array, type `MyArray = FIn.variable`. For example for the time variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Store only the values in a variable ###\n",
    "VarTime=FIn.time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VarTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert a date from a format to another, use the `astype()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#astype allows to choose the precision of the date: D for Day, Y for years, M for months, h for hours etc\n",
    "\n",
    "print(VarTime.astype('datetime64[D]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.2. Visualisation of one NO2 product\n",
    "\n",
    "In this part, we will map the product we just opened with `xarray`. Each Sentinel-5P product corresponds to one satellite track (a pole to pole half orbit). We have previously seen the exploration and manipulation of the different groups and variables in these groups. Now we would like to access the actual NO<sub>2</sub> measurement in order to plot it. This quantity is stored in the `nitrogen_tropospheric_column` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Tip:</b> To access a variable `Var` in a xarray.Dataset `FIn`, use `Myvar=FIn[Var]`. This will store the variable as an <a href=\"http://xarray.pydata.org/en/stable/data-structures.html#dataarray\"> `xarray.DataArray` </a>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Get the time variable ###\n",
    "date=VarTime.astype('datetime64[D]')\n",
    "\n",
    "###  Store the variable NO2\n",
    "VarNO2=FIn['nitrogendioxide_tropospheric_column'] #store the NO2 measurement variable of FIn in VarNO2\n",
    "\n",
    "FIn.close() #Close the xarray Dataset as we got all the needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the characteristics of VarNO2\n",
    "print(VarNO2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray provides convenient plot tools for mapping DataArray. In this notebook, we will extensively use the <a href=\"http://xarray.pydata.org/en/stable/generated/xarray.plot.pcolormesh.html\"> xarray.plot.pcolormesh() </a> function which simply wraps the <a href=\"https://matplotlib.org/api/_as_gen/matplotlib.pyplot.pcolormesh.html#matplotlib.pyplot.pcolormesh\"> matplotlib.pyplot.pcolormesh() </a> function. To improve the map of the nitrogen dioxyde column in the product, we will also use the Plate Carree projection from <a href=\"https://scitools.org.uk/cartopy/docs/latest/\"> cartopy </a>. `cartopy` is a python package designed for geospatial data processing in order to produce maps and other geospatial data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization using Cartopy and Plate Carree projection\n",
    "fig=plt.figure(figsize=(40,15)) # create a figure frame and set up the figure size\n",
    "ax = plt.axes(projection=ccrs.PlateCarree()) # Creates an empty subplot with Plate Carree projection\n",
    "\n",
    "# Plot VarNO2 where we remove the time dimension `VarNO2[0]`\n",
    "im=VarNO2[0].plot.pcolormesh(ax=ax, x='longitude', y='latitude', \n",
    "                             add_colorbar=False, cmap='jet', \\\n",
    "                           transform=ccrs.PlateCarree(),vmin=0,vmax=0.0001) \n",
    "\n",
    "# Define the figure colorbar\n",
    "cbar=fig.colorbar(im, ax=ax, pad=0.01, format='%.1e')\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "cbar.set_label(label='Tropospheric vertical column of NO2 (mol.m-2)',fontsize=16)\n",
    "\n",
    "#ax.add_feature(cartopy.feature.RIVERS) # add river\n",
    "ax.set_title('S-5p L2 NO$_2$ ({}) '.format(str(date[0])), fontsize=50) # add title\n",
    "ax.coastlines('10m')  # add coastline\n",
    "ax.stock_img() # add earth background\n",
    "ax.gridlines()  # add grid line\n",
    "\n",
    "# Save figure to file\n",
    "plt.savefig('/shared/S5P_NO2_092021/training_NO2_22092021/Processing/figures/NO2_one_track.png', facecolor='white', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just mapped one raw Sentinel-5P NO2 product using xarray and cartopy, without applying any kind of preprocessing. Different indicators (like `qa_value` introduced above) come with the data and describe the context around the measurement. Moreover, mapping only one product at a time offers limited applications. Users are more interested in getting all available samples over their region of interest for a given period of time in order to identify trends or punctual phenomena. Studying NO2 concentration over a given area with satellite data usually involves computing 10-15 day averages of the NO2 column measured in order to iron out the effects of weather.\n",
    "\n",
    "The next part describes how to spatially and temporaly filter and reproject Sentinel-5P products, how to keep only the best quality pixels and how to merge several single track products into one product. These exercises will rely on the Atmospheric toolbox and more particularly on its HARP component which was introduced to you earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualisation of NO2 for a day\n",
    " [Go back to the \"Table of contents\"](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO2 is an important trace gas in the Earth's atmosphere. It is present in both the troposphere and the stratosphere as a result of:\n",
    "   - Anthropogenic activities (traffic, industrial fossil fuel combustion, biomass burning)\n",
    "   - Natural processes (wildfires, lightning)\n",
    "\n",
    "NO2 has a short lifetime in the atmosphere (from a few hours up to a half day) and remains relatively close to its source. This is why it is often used to as a proxy to follow the evolution of Anthropogenic activies. You will find more information in the <a href=\"https://sentinel.esa.int/documents/247904/2476257/Sentinel-5P-TROPOMI-ATBD-NO2-data-products\"> TROPOMI ATBD of the tropospheric and total NO2 data products </a>\n",
    "\n",
    "In this part we analyze NO2 concentration in the world troposphere on march 24th 2019. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files have been downloaded ahead of this training session for this exercise. The command line used to download them is displayed below:\n",
    "* -E option has been updated\n",
    "* no need of the -c option as we downloaded all products acquired over the course of March 24th 2019.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Command line to download data, uncomment the -o option if you want to download the product\n",
    "!/shared/S5P_NO2_092021/training_NO2_22092021/AuxData/bash/dhusget.sh \\\n",
    "-d 'https://s5phub.copernicus.eu/dhus' \\\n",
    "-u 's5pguest' -p 's5pguest' \\\n",
    "-m 'Sentinel-5' \\\n",
    "-S 2019-03-24T00:00:00.000Z -E 2019-03-25T02:0:00.000Z \\\n",
    "-F 'platformname:Sentinel-5 AND producttype:L2__NO2___ AND processinglevel:L2 AND processingmode:Offline' \\\n",
    "-l 100 \\\n",
    "-q ../Processing/automatic_download/search_result_`date +%d%m%y`_S5P.xml \\\n",
    "-C ../Processing/automatic_download/products_list_`date +%d%m%y`_S5P.csv \\\n",
    "-O '/shared/S5P_NO2_092021/training_NO2_22092021/Processing/automatic_download/' \\\n",
    "#-o 'product'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Tip:</b> The Earth is usually covered by 14 to 15 S5P products in a single day. You may need to increase the end date by a few hours in order to find all products for a day.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To highlight the various possibilities of the Atmospheric Toolbox, this exercise presents three different and equivalent ways of merging Sentinel-5P L2 products into a gridded L3 averaged product. For each different method, we will calculate the computing time, to determine the fastest way of merging NO2 products.\n",
    "\n",
    "The characteristics of the averaged NO2 products are the following:\n",
    "* Destination grid:\n",
    "    * Lon: [-180°E,180°E] 3600 bins with a step of 0.1°;\n",
    "    * Lat: [-90°N,90°N] 1800 bins with a step of 0.1°;\n",
    "* We will filter out the observations with a quality flag less than 75. This is the recommended threshold value in the <a href=\"https://sentinel.esa.int/documents/247904/2474726/Sentinel-5P-Level-2-Product-User-Manual-Nitrogen-Dioxide\"> Sentinel-5P NO2 Product User Manual </a>. This quality indicator depends on many factor including cloud cover, surface albedo, presence of snow-ice, saturation, geometry etc. These aspects are taken into account in the definition of the \"quality assurance value\" (qa_value), available for each individual observation, which provides the users with an easy filter to remove less accurate observations. The qa_value is a continuous variable, ranging from 0 (error) to 1 (optimal retrieval). The main flag for data usage is as follows: \n",
    "    * qa_value > 0.75: This is the recommended pixel filter. It removes cloud-covered scenes (cloud radiance fraction> 0.5), partially snow/ice covered scenes, errors, and problematic retrievals.\n",
    "    * qa_value > 0.50: Compared to the stricter filter, this adds the good quality retrievals over clouds and over scenes covered by snow/ice. Errors and problematic retrievals are still filtered out. In particular, this filter may be useful for assimilation and model comparison studies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1. Harpmerge in command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method will consist in using the **harpmerge** command line tool. As we saw in the presentation, harpmerge combines multiple products from files or directory by appending them accross the time dimension and storing them into a single output file. For this we will use the command line below:\n",
    "\n",
    "Harpmerge <font color='red'>–ap ‘operations_list’</font> <font color='green'>–a ‘operations_list’ </font> <font color='blue'>–o ‘ingestion_option’ </font> <font color='black'>input_files</font> <font color='brown'> output_Directory/output_file</font>\n",
    "\n",
    "`keep()` is an operation used in this example, which specifies that all variables marked for inclusion will be kept in the ingested product, while all other variables will be excluded.\n",
    "\n",
    "To assess the processing time, we will also use the `time` method of python. This method returns the time as a floating point number expressed in seconds since the epoch, in UTC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Tip:</b> When calling a linux command from a notebook, insert a `!` at the beginning of the command line. To split a long command line into several smaller ones, use `\\` at the end of each line.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting time of the cell\n",
    "t0= time.time()\n",
    "\n",
    "#harpmerge in command line\n",
    "!harpmerge -ap 'bin(); squash(time, (latitude,longitude,tropospheric_NO2_column_number_density))' \\\n",
    "-a 'tropospheric_NO2_column_number_density_validity>75; \\\n",
    "bin_spatial(1800,-90,0.1,3600,-180,0.1); \\\n",
    "derive(longitude {longitude});derive(latitude {latitude}); \\\n",
    "keep(latitude,longitude,tropospheric_NO2_column_number_density,weight)' \\\n",
    "/shared/S5P_NO2_092021/training_NO2_22092021/Original/NO2_24032019/S5P_OFFL_L2__NO2____20190*.nc \\\n",
    "/shared/S5P_NO2_092021/training_NO2_22092021/Processing/merged_files/command_line/converted_NO2_command_line.nc\n",
    "\n",
    "#End time for command line\n",
    "t1 = time.time()\n",
    "\n",
    "command_line_time = t1-t0\n",
    "\n",
    "print(\"Command line time: {} seconds\".format(command_line_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Tip:</b> Remember that when ingesting products, HARP will rename the product variables. The HARP variable mapping for S5P is available on <a href=\"https://stcorp.github.io/harp/doc/html/ingestions/index.html#sentinel-5p-products\"> this webpage </a>. \n",
    "\n",
    "For example, the `nitrogen_tropospheric_column` variable will be labelled as `tropospheric_NO2_column_number_density` in the corresponding HARP product. \n",
    "        \n",
    "In the same way, `qa_value` will be labelled as `nitrogen_tropospheric_column_number_density_validity` and rescaled between 0 and 100 (rather than 0-1) in the corresponding HARP products\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2. Harp.import_product() without preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method will consist in using the <a href=\"https://stcorp.github.io/harp/doc/html/python.html\"> python interface of HARP </a> and the `harp.import_product()` function. We have already imported the `harp` Python library in [part 2](#2.-Importing-python-libraries).   This function works as the `harpmerge` command line tool. It takes a single input file or a set of input files. The operations to be applied to each L2 product before they are merged are specified through the `operations=` (equivalent to the `-a` of `harpmerge`) argument (should be specified as a semi-colon separated string of operations). The operations to be applied to the merged product are specified via the `post_operations=` (equivalent to the `-ap` of `harpmerge`) argument.\n",
    "\n",
    "`harp.import_product()` returns the imported or the merged product (depending on whether the input is a single file or a set of files). If you want to export the output to a NetCDF file, use the `harp.export_product()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify input and output of harp.import_product\n",
    "input_files='/shared/S5P_NO2_092021/training_NO2_22092021/Original/NO2_24032019/S5P_OFFL_L2__NO2____20190*.nc' #All S5P L2 NO2 product on 24/03/2019\n",
    "export_path='/shared/S5P_NO2_092021/training_NO2_22092021/Processing/merged_files/without_preprocessing/converted_NO2_without_preprocessing.nc' #NetCDF file where merged product will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Starting time of the cell\n",
    "t0= time.time()\n",
    "\n",
    "#Use harp.import_product python function to generate \n",
    "Converted_NO2 = harp.import_product(input_files, \\\n",
    "                      operations= \"tropospheric_NO2_column_number_density_validity>75; \\\n",
    "                      bin_spatial(1800,-90,0.1,3600,-180,0.1); \\\n",
    "                      derive(latitude {latitude}); derive(longitude {longitude}); \\\n",
    "                      keep(latitude,longitude,tropospheric_NO2_column_number_density,weight)\", \\\n",
    "                      post_operations=\"bin(); squash(time, (latitude,longitude,tropospheric_NO2_column_number_density))\"                   \n",
    "                      )\n",
    "        \n",
    "harp.export_product(Converted_NO2, export_path,file_format=\"netcdf\")\n",
    "\n",
    "#End time for merged product\n",
    "t1 = time.time()\n",
    "\n",
    "without_preprocessing_time = t1-t0\n",
    "\n",
    "print(\"time without preprocessing: {} seconds\".format(without_preprocessing_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3. Harp.import_product() with preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the 2 previous cells, generating L3 products from L2 products can take up a lot of computing time when working with many files. When exploring Sentinel-5P data, we noticed that they contain many variables and only a few of them are of interest for our study. This is why pre-processing the files before merging them can greatly optimize the performance. To this end, we use the `harp.import_product()` function on each S5P L2 NO<sub>2</sub> file to remove the variables we are not interested in and crop the full orbit files to our area of interest. This way, we will work with much lighter L2 products. \n",
    "\n",
    "In the code cell below, we first define the `input_path` and `export_path`. Then we collect in a single variable all input file paths. For this, we use the following functions:\n",
    "   - `join()`: allows to gather different parts of a path, here the path of the folder and the name of the files\n",
    "   - `glob.glob()`: returns a potentially empty list of paths matching the pattern pathname\n",
    "   - `listdir()`: returns a list containing the names of the entries in the directory given by path in arbitrary order\n",
    "   - `sorted()`: returns a sorted list of the specified iterable object\n",
    "\n",
    "We ill use the `list_files` variable to store each pre-processed file with the same name as its corresponding input file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>WARNING</b>\n",
    "    \n",
    "***  \n",
    "\n",
    "In the following cell we will use a for loop. It is important to keep in mind that the Python language starts at index 0 (0-based). For example, `files_input[0:3]` will return the values of the `files_input` variable from index 0 to 2. Python does not take into account the last index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path= '/shared/S5P_NO2_092021/training_NO2_22092021/Original/NO2_24032019/' #Directory of the original S5P files\n",
    "export_path='/shared/S5P_NO2_092021/training_NO2_22092021/Processing/pre_processed_files/NO2_24032019' #output directory\n",
    "\n",
    "# the function `list` recovers the name of all files in the input_path folder\n",
    "list_files = sorted(os.listdir(input_path))\n",
    "\n",
    "# Make sure that list_files contains only S5P files (if the input directory contains other files)\n",
    "for file in list_files:\n",
    "    if file.startswith(\"S5P_OFFL_\")==False:\n",
    "        list_files.remove(file)\n",
    "\n",
    "# Get the name of all files in the folder and sort them in alphabetic order\n",
    "files_input= sorted(glob.glob(join(input_path, 'S5P_OFFL_*.nc')))\n",
    "\n",
    "#Starting time of the preprocessing\n",
    "t0 = time.time()\n",
    "\n",
    "#Pre-processing loop on each file in input_path to get rid of unnecessary variables (keep()) and crop to our ROI\n",
    "for i in range(len(files_input)):\n",
    "    #Pre-processing\n",
    "    Converted_NO2=harp.import_product(files_input[i], \\\n",
    "            operations= \"keep(latitude,latitude_bounds,longitude,longitude_bounds, \\\n",
    "            tropospheric_NO2_column_number_density,tropospheric_NO2_column_number_density_validity,datetime_start)\")\n",
    "    #Export of the preprocessed file to export_path\n",
    "    harp.export_product(Converted_NO2, join(export_path, list_files[i]),file_format=\"netcdf\") \n",
    "    print(\"product\", os.path.basename(files_input[i]),\"pre-processed\")\n",
    "\n",
    "#End time of the pre-precessing\n",
    "t1 = time.time()\n",
    "\n",
    "pre_processing_time = t1-t0\n",
    "\n",
    "print(\"Pre-processing time: {} seconds\".format(pre_processing_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's merge together the pre-processed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input and output of harp.import_product\n",
    "input_files='/shared/S5P_NO2_092021/training_NO2_22092021/Processing/pre_processed_files/NO2_24032019/S5P_OFFL_L2__NO2_*.nc' #All S5P preprocessed product\n",
    "export_file='/shared/S5P_NO2_092021/training_NO2_22092021/Processing/merged_files/with_preprocessing/converted_NO2_with_preprocessing.nc' #outputfile\n",
    "\n",
    "#Use harp.import_product python function to generate the merged product\n",
    "\n",
    "#Start time of the processing\n",
    "t0 = time.time()\n",
    "\n",
    "#Merging of the pre-processed files\n",
    "Converted_NO2 = harp.import_product(input_files, \\\n",
    "                      operations= \"tropospheric_NO2_column_number_density_validity>75; \\\n",
    "                      bin_spatial(1800,-90,0.1,3600,-180,0.1); \\\n",
    "                      derive(latitude {latitude}); derive(longitude {longitude}); \\\n",
    "                      keep(latitude,longitude,tropospheric_NO2_column_number_density,weight)\", \\\n",
    "                      post_operations=\"bin(); squash(time, (latitude,longitude,tropospheric_NO2_column_number_density))\"                   \n",
    "                      )\n",
    "\n",
    "#Export of the merged file        \n",
    "harp.export_product(Converted_NO2, export_file,file_format=\"netcdf\")\n",
    "\n",
    "#End time of the processing\n",
    "t1 = time.time()\n",
    "\n",
    "processing_time = t1-t0\n",
    "\n",
    "#Total processing time accounts for the preprocessing time and the actual merging\n",
    "total_processing_time=pre_processing_time+processing_time\n",
    "\n",
    "print(\"Processing time: {} seconds\".format(processing_time))\n",
    "print(\"Total processing time with preprocessing: {} seconds\".format(total_processing_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, the computing time is much shorter when pre-processing the NO2 files beforehand. Remember, we have preprocessed the original files by reducing the number of variables.\n",
    "\n",
    "It is also possible to restrict the geographical extent to a square above your region of interest \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> When running HARP, different Exceptions can take place. (click <a href='https://stcorp.github.io/harp/doc/html/python.html#examples'> here </a> for more details on HARP Exceptions). One of the most common you may face is the <i>NoDataError</i>, which is raised when the product returned from an import contains no variables, or variables without data. This can especially happen when importing single products. This can be caused for example by a product that does not overlap with your AOI or because all ground pixels have been filtered out by the <i>qa_value</i>. \n",
    " \n",
    "This kind of Exception can be caught with `try:` and `except NoDataError:` so that the processing keeps working if a `NoDataError` exception is raised\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Visualisation of NO2 \n",
    "\n",
    "Now that we have created the merged products on 24th march 2019 using 3 differents approaches, we can map the averaged NO2 column and check that the merged products are the same.\n",
    "\n",
    "Before opening the merged products we `def`ine a python function named `figures()`. This way we will be able to generate figures by simply calling the `figure` function rather than repeating the same code.\n",
    "\n",
    "This function takes as input the following variables:\n",
    "* data: the data that we want to visualize (must be an `xr.DataArray`)\n",
    "* ax: the axe used for the figure (a `plt.subplot` with a defined projection)\n",
    "* title: title of the figure (string)\n",
    "* shrink: the size of the colorbar between 0 and 1 (float)\n",
    "\n",
    "We will use the `pcolormesh()` function for the colour mapping.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figures(data:xr.DataArray,ax:plt.subplot,title:str, shrink:float, alpha:float) -> plt.subplot:\n",
    "    \"\"\"\n",
    "    Creates a coloured map of NO2 tropospheric column. \n",
    "    Args:\n",
    "        (xr.dataArray) data, NO2 tropospheric column\n",
    "        (subplot) ax, supblot with a defined projection\n",
    "        (str) title, title of your figure\n",
    "        (float) shrink, size of color bar\n",
    "        (float) alpha, The alpha blending value, between 0 (transparent) and 1 (opaque)\n",
    "    Returns:\n",
    "        (subplot) coloured map of the NO2 tropospheric column\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    # Coloured Map of O3 total column levels\n",
    "    im1=data.plot.pcolormesh(ax=ax, x='longitude', y='latitude',add_colorbar=False, \n",
    "                            cmap='jet', transform=ccrs.PlateCarree(), vmin=0, vmax=0.12, alpha=alpha)\n",
    "    \n",
    "    # Figure settings\n",
    "    ax.set_title(title ,fontsize=35)\n",
    "    ax.add_feature(cartopy.feature.RIVERS)\n",
    "    ax.coastlines('10m',linewidth=2)\n",
    "    ax.stock_img() # add earth background\n",
    "    ax.gridlines()\n",
    "    cbar = plt.colorbar(im1,ax=ax, orientation=\"vertical\",pad=0,shrink=shrink)#pad adjust the space between plots and colorbar\n",
    "    cbar.ax.tick_params(labelsize=20)\n",
    "    \n",
    "    return (ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1. Visualisation of the three outputs\n",
    "\n",
    "First, we will open each merged product with the cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the data is stored\n",
    "FName_command_line='/shared/S5P_NO2_092021/training_NO2_22092021/Processing/merged_files/command_line/converted_NO2_command_line.nc'\n",
    "FName_without_preprocessing='/shared/S5P_NO2_092021/training_NO2_22092021/Processing/merged_files/without_preprocessing/converted_NO2_without_preprocessing.nc'\n",
    "FName_with_preprocessing='/shared/S5P_NO2_092021/training_NO2_22092021/Processing/merged_files/with_preprocessing/converted_NO2_with_preprocessing.nc'\n",
    "\n",
    "# Put the data in the Datasets\n",
    "FIn_cl = xr.open_dataset(FName_command_line)\n",
    "FIn_noprep = xr.open_dataset(FName_without_preprocessing)\n",
    "FIn_prep = xr.open_dataset(FName_with_preprocessing)\n",
    "# Put the tropospheric NO2 data in a variable\n",
    "# We mutiply by 1000 to convert from [mol.m-2] to [mmol.m-2] \n",
    "VarNO2_cl=FIn_cl['tropospheric_NO2_column_number_density']*(10**3)\n",
    "VarNO2_noprep=FIn_noprep['tropospheric_NO2_column_number_density']*(10**3)\n",
    "VarNO2_prep=FIn_prep['tropospheric_NO2_column_number_density']*(10**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates one figure with three subplots on the same column (one subplot per approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ax definition\n",
    "fig,axes=plt.subplots(3,1,figsize=(30,50)) # create a figure frame with three lines and one column and set up the figure size\n",
    "axes=axes.ravel() #Turn the axes into a 1D array in order to loop on the different axes\n",
    "titles=['With command line','Without preprocessing','With preprocessing'] #Array of the titles\n",
    "variables=[VarNO2_cl,VarNO2_noprep,VarNO2_prep] #Array of the variables to be plotted\n",
    "shrink=0.8\n",
    "alpha=0.8\n",
    "\n",
    "for i in range(0,len(axes)):\n",
    "    j=311+i #to define the ax position in the subplot 311, 312 and 313\n",
    "    axes[i]=plt.subplot(j,projection=ccrs.PlateCarree()) # Creates an empty subplot with Orthographic projection\n",
    "\n",
    "    #Title definition\n",
    "    title =titles[i] # title of the current ax\n",
    "\n",
    "    #Function call\n",
    "    figures(variables[i],axes[i],title,shrink,alpha) #figures function call on the current ax for the current variable\n",
    "\n",
    "#Define the general title\n",
    "fig.suptitle('S-5p averaged NO$_2$ tropospheric column on 24/03/2019(mmol.m-2)',fontsize=35,y=0.88)\n",
    "#Adjust the vertical space between subplots\n",
    "fig.subplots_adjust(hspace=0.01)\n",
    "# Save figure to file\n",
    "plt.savefig('/shared/S5P_NO2_092021/training_NO2_22092021/Processing/figures/NO2_three_methods', facecolor='white',\\\n",
    "            bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above shows three identicals figures. This indicates that the three approaches processed the input products in an equivalent way.\n",
    "\n",
    "We notice some data gaps on our maps. Indeed, in the previous section, when generrating the daily merged products, we discarded the lowest quality pixels. Hence, harp did not take into account the pixels with a `qa_value<0.75` (or a `tropospheric_NO2_column_number_density_validity<75` in harp convention). These pixels were most probably covered in clouds or were suffering from a data retrieval issue.  This is why it is advised to study the NO2 averaged tropospheric column over a 2 week period. This ensures the production of gap-free maps and this irons out the effect of weather on the NO2 tropospheric column. \n",
    "Indeed, we are interested in studying the anthropogenic emissions of NO2. Focusing on only one day at a time could be misleading as the tropospheric NO2 can be dispersed by winds or our area of interest covered in clouds when the measurements were collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a further assessment we can directly chech the equality of the three generated products thanks to the `xarray.dataArray.equals()` function. This function will return `True` if two DataArrays have the same dimensions, coordinates and values; `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal1=VarNO2_cl.equals(VarNO2_noprep)\n",
    "equal2=VarNO2_noprep.equals(VarNO2_prep)\n",
    "if (equal1 and equal2):\n",
    "    print(\"The three products are equivalent!\")\n",
    "else:\n",
    "    print(\"The three products are not equivalent...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. Visualisation of NO2 over Europe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we are going to focus over Europe. For this, we will crop the product to an area of interest defined by a latitude range [31;58]°N and a longitude range [-34;34]°E. We use the `sel()` function (more information [here](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.sel.html)). The function returns a new dataset with each array indexed by tick labels along the specified dimension(s), here the `longitude` & `latitude` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deine lat and lon extent\n",
    "lon_min=-34\n",
    "lon_max=34\n",
    "lat_min=32\n",
    "lat_max=58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO2_Europe=VarNO2_prep.sel(latitude=slice(lat_min,lat_max),longitude=slice(lon_min,lon_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates one figure with two subplots. The first image (left subplot) is the tropospheric NO2 on 24/03/2019 over the world with our area of interest highlighted by the red rectangle. The second image (right subplot) is the tropospheric NO2 on 24/03/2019 over Europe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrink=0.4\n",
    "alpha=0.6\n",
    "\n",
    "fig,(axe1,axe2)=plt.subplots(1,2,figsize=(30,15))\n",
    "\n",
    "axe1 = plt.subplot(121, projection=ccrs.PlateCarree())\n",
    "title = 'World'\n",
    "figures(VarNO2_prep,axe1,title,shrink,alpha)\n",
    "# add the red rectangle, that corresponds to the area of the zoom on Europe\n",
    "axe1.add_patch(mpatches.Rectangle(xy=[lon_min, lat_min], width=lon_max-lon_min, height=lat_max-lat_min,\n",
    "                                    edgecolor='red', facecolor='None', linewidth=4,\n",
    "                                    transform=ccrs.PlateCarree()))\n",
    "\n",
    "axe2 = plt.subplot(122, projection=ccrs.PlateCarree())\n",
    "title = 'zoom over Europe'\n",
    "figures(NO2_Europe,axe2,title,shrink,alpha)\n",
    "axe2.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "\n",
    "#Define the general title\n",
    "fig.suptitle('Tropospheric NO2 column on 24/03/2019 (mmol.m-2)',fontsize=35,y=0.73)\n",
    "\n",
    "# Save figure to file\n",
    "plt.savefig('/shared/S5P_NO2_092021/training_NO2_22092021/Processing/figures/NO2_24032019_zoom_in_Europe', facecolor='white'\n",
    "            \\ bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the data gaps are still present, some areas show higher concentrations of NO2:\n",
    "* Barcelona,Spain\n",
    "* Near Marseille,France\n",
    "* The region of Lombardy in Italy\n",
    "* The region of Ruhr in Germany\n",
    "* Londonian area, United Kingdom\n",
    "\n",
    "In the second part of this training, we will compare the tropospheric column of NO2 over Europe between 2019 and 2020 for a defined period. Indeed, starting mid March 2020, there were quarantines or similar restrictions (variously described as stay-at-home orders, shelter-in-place orders, shutdowns or lockdowns) happening in many countries and territories around the world, related to the COVID-19 pandemic and established to prevent the further spread of the virus. In particular, Europe's population was under lockdown, having been asked or ordered to stay at home by their governments.\n",
    "\n",
    "During this lockdown, air quality over most European cities improved significantly due to the dramatic drop of traffic and industrial activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    " [Go back to the \"Table of contents\"](#Table-of-contents)\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>CONGRATULATIONS</b><br>\n",
    "  \n",
    "--- \n",
    "\n",
    "#### And thank you for your attention! :) We hope you enjoyed this training on Sentinel 5P provided by RUS - Copernicus for free, thanks to ESA and the European Commission.\n",
    "\n",
    "#### Now let's try to download new data and variables and to access and visualize them. You can try to make new maps and plots. And don't forget to try the other [Sentinel 5P products](https://s5phub.copernicus.eu/dhus/)!\n",
    "\n",
    "This training course is over but we'd love to hear from you about how we could improve it (topics, tools, storytelling, format, speed etc). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
