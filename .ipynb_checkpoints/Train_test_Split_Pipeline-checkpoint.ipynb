{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed1ff398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba76321e",
   "metadata": {},
   "source": [
    "### Open the hashmap file for train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d165b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker'\n",
    "hashdict_filename = 'landfill_hashdict.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb047daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashdict(path, hashdict_filename):\n",
    "    try: \n",
    "        with open(os.path.join(path, hashdict_filename), \"rb\") as hashed_l: # check if the hashed location file exists  \n",
    "                    hashed_locations = pickle.load(hashed_l) # load dictionary object\n",
    "        return hashed_locations\n",
    "    except:\n",
    "        print(\"Check the filename and filepath: not found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7ca003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_dict = get_hashdict(path=path, hashdict_filename=hashdict_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a4aa944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2293"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hash_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99e176",
   "metadata": {},
   "source": [
    "### Split the hashmaps in train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fbc49a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_sets(d, test_size=0.05):\n",
    "    total_size = len(d)\n",
    "    \n",
    "    if type(test_size) == float:\n",
    "        size = int(total_size * test_size)\n",
    "    elif type(test_size) == int:\n",
    "        size = test_size\n",
    "    else:\n",
    "        print(\"please check the test size parameter\")\n",
    "        return\n",
    "    keys = random.sample(list(d.keys()), size)\n",
    "    \n",
    "    test_sample = {k: d[k] for k in keys}\n",
    "    train_sample = {k:d[k] for k in d.keys() if k not in keys}\n",
    "    \n",
    "    return train_sample, test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f24c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hashmap, test_hashmap = create_train_test_sets(hash_dict, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ee1b751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2064\n",
      "Test set size: 229\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size: {}\".format(len(train_hashmap)))\n",
    "print(\"Test set size: {}\".format(len(test_hashmap)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f34b83",
   "metadata": {},
   "source": [
    "### Saving the train and test hashmap for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59fca1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_test_dict (path, datadict ,filenames = ['training_datadict.pkl', 'test_datadict.pkl']):\n",
    "    \"\"\"\n",
    "    path: str(path where we want to save the dictionaries)\n",
    "    datadict: list(dict) (list of dictionaries containing traning and test dict)\n",
    "    filenames: list(str) (list of training and test filenames)\n",
    "    \n",
    "    return None\n",
    "    \"\"\"\n",
    "    \n",
    "    for itr, filename in enumerate(filenames):\n",
    "        with open(os.path.join(path, filename), \"wb\") as fw:\n",
    "            pickle.dump(datadict[itr], fw, protocol=pickle.HIGHEST_PROTOCOL) # rewrite the most updated file to disk \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1992b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_train_test_dict(path = path, datadict = [train_hashmap, test_hashmap])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ab7481",
   "metadata": {},
   "source": [
    "### Moving training and test files in appropriate folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b627f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker/training_data/'\n",
    "test_data_path = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker/test_data/'\n",
    "base_image_repo = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker/complete_image_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36d8da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(img_name):\n",
    "    elems = img_name.split('_')\n",
    "    if len(elems) == 2:\n",
    "        return int(elems[1][:-8])\n",
    "    else:\n",
    "        return int(elems[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a832317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_train_test_data(base_image_repo, train_data_path, test_data_path, test_datadict, correct_filename=False):\n",
    "    \n",
    "    for itr, fname in enumerate(os.listdir(base_image_repo)):\n",
    "        \n",
    "        if itr % 100 == 0:\n",
    "            print('{} files complete'.format(itr))\n",
    "        \n",
    "        #print(fname)\n",
    "    \n",
    "        if correct_filename:\n",
    "            corrected_fname = fname[:-4]\n",
    "\n",
    "        hashmap = get_hash(fname)\n",
    "        src = base_image_repo+fname\n",
    "\n",
    "        if hashmap in test_datadict:\n",
    "            dest = test_data_path+corrected_fname\n",
    "            os.rename(src, dest)\n",
    "        else:\n",
    "            dest = train_data_path+corrected_fname\n",
    "            os.rename(src, dest)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0351bfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files complete\n",
      "100 files complete\n",
      "200 files complete\n",
      "300 files complete\n",
      "400 files complete\n",
      "500 files complete\n",
      "600 files complete\n",
      "700 files complete\n",
      "800 files complete\n",
      "900 files complete\n",
      "1000 files complete\n",
      "1100 files complete\n",
      "1200 files complete\n",
      "1300 files complete\n",
      "1400 files complete\n",
      "1500 files complete\n",
      "1600 files complete\n",
      "1700 files complete\n",
      "1800 files complete\n",
      "1900 files complete\n",
      "2000 files complete\n",
      "2100 files complete\n",
      "2200 files complete\n",
      "2300 files complete\n",
      "2400 files complete\n",
      "2500 files complete\n",
      "2600 files complete\n",
      "2700 files complete\n",
      "2800 files complete\n",
      "2900 files complete\n",
      "3000 files complete\n",
      "3100 files complete\n",
      "3200 files complete\n",
      "3300 files complete\n",
      "3400 files complete\n",
      "3500 files complete\n",
      "3600 files complete\n",
      "3700 files complete\n",
      "3800 files complete\n",
      "3900 files complete\n",
      "4000 files complete\n",
      "4100 files complete\n",
      "4200 files complete\n",
      "4300 files complete\n",
      "4400 files complete\n",
      "4500 files complete\n",
      "4600 files complete\n",
      "4700 files complete\n",
      "4800 files complete\n",
      "4900 files complete\n",
      "5000 files complete\n",
      "5100 files complete\n",
      "5200 files complete\n",
      "5300 files complete\n",
      "5400 files complete\n",
      "5500 files complete\n",
      "5600 files complete\n",
      "5700 files complete\n",
      "5800 files complete\n",
      "5900 files complete\n",
      "6000 files complete\n",
      "6100 files complete\n",
      "6200 files complete\n",
      "6300 files complete\n",
      "6400 files complete\n",
      "6500 files complete\n",
      "6600 files complete\n",
      "6700 files complete\n",
      "6800 files complete\n",
      "6900 files complete\n",
      "7000 files complete\n",
      "7100 files complete\n",
      "7200 files complete\n",
      "7300 files complete\n",
      "7400 files complete\n",
      "7500 files complete\n",
      "7600 files complete\n",
      "7700 files complete\n",
      "7800 files complete\n",
      "7900 files complete\n",
      "8000 files complete\n",
      "8100 files complete\n",
      "8200 files complete\n",
      "8300 files complete\n",
      "8400 files complete\n",
      "8500 files complete\n",
      "8600 files complete\n",
      "8700 files complete\n",
      "8800 files complete\n",
      "8900 files complete\n",
      "9000 files complete\n",
      "9100 files complete\n",
      "9200 files complete\n",
      "9300 files complete\n",
      "9400 files complete\n",
      "9500 files complete\n",
      "9600 files complete\n",
      "9700 files complete\n",
      "9800 files complete\n",
      "9900 files complete\n",
      "10000 files complete\n",
      "10100 files complete\n",
      "10200 files complete\n",
      "10300 files complete\n",
      "10400 files complete\n",
      "10500 files complete\n",
      "10600 files complete\n",
      "10700 files complete\n",
      "10800 files complete\n",
      "10900 files complete\n",
      "11000 files complete\n",
      "11100 files complete\n",
      "11200 files complete\n",
      "11300 files complete\n",
      "11400 files complete\n",
      "11500 files complete\n",
      "11600 files complete\n",
      "11700 files complete\n",
      "11800 files complete\n",
      "11900 files complete\n",
      "12000 files complete\n",
      "12100 files complete\n",
      "12200 files complete\n",
      "12300 files complete\n",
      "12400 files complete\n",
      "12500 files complete\n",
      "12600 files complete\n",
      "12700 files complete\n",
      "12800 files complete\n",
      "12900 files complete\n",
      "13000 files complete\n"
     ]
    }
   ],
   "source": [
    "move_train_test_data(base_image_repo=base_image_repo, \n",
    "                     train_data_path=train_data_path, \n",
    "                     test_data_path=test_data_path, \n",
    "                     test_datadict=test_hashmap, \n",
    "                     correct_filename=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
