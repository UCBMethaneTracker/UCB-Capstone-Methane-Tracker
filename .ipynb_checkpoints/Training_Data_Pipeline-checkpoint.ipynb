{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6bd79e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import os\n",
    "import ee\n",
    "import geemap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import skimage.transform\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9239a54",
   "metadata": {},
   "source": [
    "### Earth Engine Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8254b452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=sI66S7SrtKY-ikdFFYAjNU1R2Ac8YhH0tohWPd5LxM8&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=sI66S7SrtKY-ikdFFYAjNU1R2Ac8YhH0tohWPd5LxM8&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter verification code: 4/1AX4XfWjwHouwePZunWXuK-JwyokSZQ4IFDgtNlqa41GXBt1C2AwKaLDHlBU\n",
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb529b",
   "metadata": {},
   "source": [
    "## Key Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf84839",
   "metadata": {},
   "source": [
    "### 1. Perturbation for image centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b21fc7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb():\n",
    "    _epsilon_long = [0.005, 0.004, 0.002, 0.001, 0, -0.001, -0.002, -0.004, -0.005]\n",
    "    _epsilon_lat = [0.005, 0.004, 0.002, 0.001, 0, -0.001, -0.002, -0.004, -0.005]\n",
    "    return np.random.choice(_epsilon_long, 1, replace=False)[0], np.random.choice(_epsilon_lat, 1, replace=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f307a",
   "metadata": {},
   "source": [
    "### 2. Sampling negative cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22fd418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_negative(long, lat):\n",
    "    \"\"\"\n",
    "    long = float (incoming longitute of landfill)\n",
    "    lat = float (incoming lattitude of landfill)\n",
    "    \n",
    "    return list of tuples of size 4 (long, lat) coordinates, nearby the landfill\n",
    "    \"\"\"\n",
    "    _epsilon = 0.4\n",
    "    \n",
    "    _tup_1 = (long + _epsilon , lat + _epsilon)\n",
    "    _tup_2 = (long + _epsilon , lat - _epsilon)\n",
    "    _tup_3 = (long - _epsilon , lat + _epsilon)\n",
    "    _tup_4 = (long - _epsilon , lat - _epsilon)\n",
    "    \n",
    "    return _tup_1, _tup_2, _tup_3, _tup_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb5421",
   "metadata": {},
   "source": [
    "### 3.Get the image for a given coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639dd215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ee_image_S2(long, lat, start_date = '2020-09-01', end_date = '2021-09-21', band_list = ['B4', 'B3', 'B2']):\n",
    "    \"\"\"\n",
    "    long = float (incoming longitute of landfill)\n",
    "    lat = float (incoming lattitude of landfill)\n",
    "    start_date = string (start date for image sampling)\n",
    "    end_date = string (end date for image sampling)\n",
    "    band_list = list(string) (list of band strings to be extracted)\n",
    "    \n",
    "    return geemap image object given the long, lat\n",
    "    \"\"\"\n",
    "    # Half width of geometry region\n",
    "    _HW = 0.012\n",
    "\n",
    "    point = ee.Geometry.Point(long, lat)\n",
    "    aoi = ee.Geometry.Polygon([[\n",
    "        [long - _HW, lat + _HW],\n",
    "        [long - _HW, lat - _HW],\n",
    "        [long + _HW, lat - _HW],\n",
    "        [long + _HW, lat + _HW],\n",
    "    ]], None, False)\n",
    "\n",
    "    img_col = (\n",
    "        ee\n",
    "        .ImageCollection('COPERNICUS/S2_SR') # load from the Sentinel2 data source\n",
    "        .filterBounds(point)\n",
    "        .filterDate(start_date, end_date) \n",
    "        .sort('CLOUDY_PIXEL_PERCENTAGE') # sort ascending by the cloudy pixel percentage\n",
    "        .select(band_list)\n",
    "    )\n",
    "    \n",
    "    num_imgs = img_col.size().getInfo()\n",
    "    if num_imgs == 0:\n",
    "        return None\n",
    "\n",
    "    img = (\n",
    "        img_col\n",
    "        .first() # get the lest cloudy image\n",
    "        .clipToBoundsAndScale(\n",
    "                geometry=aoi,\n",
    "                width=512,\n",
    "                height=512,\n",
    "            )\n",
    "    )\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c790c761",
   "metadata": {},
   "source": [
    "### 4.Reading the csv with locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23fdf746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the csv needs to at least contain two columns with the column names \n",
    "# \"Latitude\" and \"Longitude\" indicating the positions of the landfills\n",
    "\n",
    "def get_landfills(filename, hashdict_filename = 'landfill_hashdict.pkl', path='D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker'):\n",
    "    \"\"\"\n",
    "    filename: string (name of the csv file containing the lattitide and longitude information)\n",
    "    hashdict_filename: string (name of the file where existing hashmap of landfill locations exist)\n",
    "    path: string (path of the directory where both files are saved)\n",
    "    \n",
    "    return dictionary (landfill locations with values as (long,lat) tuple and key as their hashmap)\n",
    "    \"\"\"\n",
    "    \n",
    "    filepath = os.path.join(path, filename) # path of the location data file\n",
    "    df = pd.read_csv(filepath) # loading on a data frame\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(path, hashdict_filename), \"rb\") as hashed_l: # check if the hashed location file exists  \n",
    "            hashed_locations = pickle.load(hashed_l) # load dictionary object\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        hashed_locations = {} # if file is not available, instnatiate wit a blank dictionary\n",
    "    \n",
    "    for index in range(len(df)):\n",
    "        lat = df.loc[index, \"Latitude\"] # \"Lattitude column name is hard coded\"\n",
    "        long = df.loc[index, \"Longitude\"] # \"Longitude column name is hard coded\"\n",
    "        \n",
    "        if (not lat) or (not long):\n",
    "            continue\n",
    "        \n",
    "        _tup = (long, lat)\n",
    "        hashmap = hash(_tup) # Create the hash of the tuple (long, lat)\n",
    "        \n",
    "        if hashmap in hashed_locations: \n",
    "            continue  # If the hash exists in the distionary then move on to avoid double counting \n",
    "        else: \n",
    "            hashed_locations[hashmap] = _tup # otherwise add the location to the dictionary\n",
    "    \n",
    "    with open(os.path.join(path, hashdict_filename), \"wb\") as fw:\n",
    "        pickle.dump(hashed_locations, fw, protocol=pickle.HIGHEST_PROTOCOL) # rewrite the most updated file to disk \n",
    "    \n",
    "    return hashed_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40839bd1",
   "metadata": {},
   "source": [
    "### 5.Save the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826fcda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(img, path, img_filename):\n",
    "    \n",
    "    filepath = os.path.join(path, img_filename)\n",
    "    \n",
    "    try: \n",
    "        img_tensor = geemap.ee_to_numpy(img)\n",
    "        img_tensor = img_tensor.clip(0, 3000) / 3000\n",
    "    \n",
    "    except:\n",
    "        print(\"Skipping: image not found\")\n",
    "        return\n",
    "    \n",
    "    plt.imsave(f'{filepath}.png', img_tensor)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a55d68",
   "metadata": {},
   "source": [
    "### 6.Check if a given location is already in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17315e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_availability(image_folder, hashmap):\n",
    "    \"\"\"\n",
    "    image_folder: str (path to the folder where all training images are saved)\n",
    "    hashmap: int (the hashed value of the (long, lat) tuple of landfill location)\n",
    "    \n",
    "    return boolean (True if the base file exists)\n",
    "    \"\"\"\n",
    "    base_filename = 'BASE_'+str(hashmap)+'.png.png'\n",
    "    pathToFile = os.path.join(image_folder, base_filename)\n",
    "    \n",
    "    return os.path.exists(pathToFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4dc3a",
   "metadata": {},
   "source": [
    "## Full processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8f50d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key variables\n",
    "landfil_locations_filename = 'consolidated_us_landfill_lat_long.csv'\n",
    "hashed_location_filename = 'landfill_hashdict.txt'\n",
    "main_directory_path = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker'\n",
    "image_file_location = 'D:/MIDS/CAPSTONE/UCB-Capstone-Methane-Tracker/complete_image_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0eb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the Landfill Dictionary \n",
    "\n",
    "landfill_locations = get_landfills(filename = landfil_locations_filename, \n",
    "                                   hashdict_filename = hashed_location_filename, \n",
    "                                   path=main_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefe008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Total landfills completed : 0\n",
      "Image.sampleRectangle: Fully masked pixels / pixels outside of the image footprint when sampling band 'B4' with no default value set. Note that calling sampleRectangle() on an image after ee.Image.clip() may result in a sampling bounding box outside the geometry passed to clip().\n",
      "Skipping: image not found\n",
      "Image.sampleRectangle: Fully masked pixels / pixels outside of the image footprint when sampling band 'B3' with no default value set. Note that calling sampleRectangle() on an image after ee.Image.clip() may result in a sampling bounding box outside the geometry passed to clip().\n",
      "Skipping: image not found\n"
     ]
    }
   ],
   "source": [
    "# Step 2: extract the images\n",
    "count = 0 \n",
    "\n",
    "for hm, tup in landfill_locations.items():\n",
    "    \n",
    "    if count%10 == 0:\n",
    "        print(\"Total landfills completed : \" + str(count))\n",
    "\n",
    "    long, lat = tup\n",
    "    if (not long) or (not lat):\n",
    "        continue \n",
    "    \n",
    "    # check if the base image exists \n",
    "    if check_availability(image_folder = image_file_location, hashmap = hm):\n",
    "        count += 1\n",
    "        continue\n",
    "    else: \n",
    "        perturb_long, perturb_lat = perturb() # sampling small perturbation\n",
    "        long_center = long + perturb_long # adjusting the center longitide\n",
    "        lat_center = lat + perturb_lat # adjusting the center lattitude\n",
    "        \n",
    "        n1, n2, n3, n4 = sample_negative(long=long, lat=lat) # Sampling nearby non-landfill containing regions\n",
    "        \n",
    "        try: \n",
    "            base_image_center = get_ee_image_S2(long=long, lat=lat) # base centered image \n",
    "            base_image = get_ee_image_S2(long=long_center, lat=lat_center) # extarct off centered base image (positive lable)\n",
    "\n",
    "            neg_samp1 = get_ee_image_S2(long=n1[0], lat=n1[1]) # extract negative lables from nearby region\n",
    "            neg_samp2 = get_ee_image_S2(long=n2[0], lat=n2[1])\n",
    "            neg_samp3 = get_ee_image_S2(long=n3[0], lat=n3[1])\n",
    "            neg_samp4 = get_ee_image_S2(long=n4[0], lat=n4[1])\n",
    "\n",
    "            base_centered_img_name = 'BASE_'+str(hm)+'_CENT_.png' \n",
    "            base_img_name = 'BASE_'+str(hm) +'.png' # follow similar naming convention for positive and negative classes\n",
    "            neg_samp1_img_name = 'NEGSAMP_'+str(hm)+'_pp.png'\n",
    "            neg_samp2_img_name = 'NEGSAMP_'+str(hm)+'_pm.png'\n",
    "            neg_samp3_img_name = 'NEGSAMP_'+str(hm)+'_mp.png'\n",
    "            neg_samp4_img_name = 'NEGSAMP_'+str(hm)+'_mm.png'\n",
    "\n",
    "            save_img(img = base_image_center, path = image_file_location, img_filename = base_centered_img_name) \n",
    "            save_img(img = base_image, path = image_file_location, img_filename = base_img_name) \n",
    "            save_img(img = neg_samp1, path = image_file_location, img_filename = neg_samp1_img_name)\n",
    "            save_img(img = neg_samp2, path = image_file_location, img_filename = neg_samp2_img_name)\n",
    "            save_img(img = neg_samp3, path = image_file_location, img_filename = neg_samp3_img_name)\n",
    "            save_img(img = neg_samp4, path = image_file_location, img_filename = neg_samp4_img_name)\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        count += 1\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
